{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error,r2_score\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblib import dump, load\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapting datasets to be interpreted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = pd.read_csv(\"data/ais_train.csv\", delimiter='|')\n",
    "X_sample.to_csv('data/ais_train_modified.csv', index=False)\n",
    "extra_vessels = pd.read_csv(\"data/vessels.csv\", on_bad_lines='skip', delimiter='|')\n",
    "extra_vessels.to_csv('data/vessels_modified.csv', index=False)\n",
    "extra_ports = pd.read_csv(\"data/ports.csv\", on_bad_lines='skip', delimiter='|')\n",
    "extra_ports.to_csv('data/ports_modified.csv', index=False)\n",
    "extra_schedules = pd.read_csv(\"data/schedules_to_may_2024.csv\", on_bad_lines='skip', delimiter='|')\n",
    "extra_schedules.to_csv('data/schedules_to_may_2024_modified.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_evaluation = pd.read_csv(\"data/ais_test.csv\",)\n",
    "extra_ports = pd.read_csv(\"data/ports_modified.csv\")\n",
    "extra_vessels = pd.read_csv(\"data/vessels_modified.csv\")\n",
    "extra_schedules = pd.read_csv(\"data/schedules_to_may_2024_modified.csv\")\n",
    "X_original = pd.read_csv(\"data/ais_train_modified.csv\")\n",
    "X_original['etaRaw'] = pd.to_datetime('2024-' + X_original['etaRaw']+ ':00', format='%Y-%m-%d %H:%M:%S',errors='coerce')\n",
    "X_original = X_original.dropna(subset=['etaRaw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing current data into previous data for the train file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def past_course(original):\n",
    "    original=original.reset_index()\n",
    "\n",
    "    original['prev_lat'] = original['latitude'].shift(1).fillna(original['latitude'].iloc[0])\n",
    "    original['prev_lon'] = original['longitude'].shift(1).fillna(original['longitude'].iloc[0])\n",
    "    original['time_2'] = original['time'].shift(1).fillna(original['time'].iloc[0])\n",
    "    original['cog'] = original['cog'].shift(1)\n",
    "    original['sog'] = original['sog'].shift(1)\n",
    "    original['rot'] = original['rot'].shift(1)\n",
    "    original['heading'] = original['heading'].shift(1)\n",
    "    original['navstat'] = original['navstat'].shift(1)\n",
    "    original.loc[0,['cog','sog','rot','heading','navstat']]=[0,0,0,0,0]\n",
    "\n",
    "    return original\n",
    "\n",
    "def two_days_past_course(original):\n",
    "\n",
    "    original= original.reset_index(drop=True)\n",
    "\n",
    "    original['prev_lat'] = original['latitude'].shift(100)\n",
    "    original['prev_lon'] = original['longitude'].shift(100)\n",
    "    original['time_2'] = original['time'].shift(100)\n",
    "    original['cog'] = original['cog'].shift(100)\n",
    "    original['sog'] = original['sog'].shift(100)\n",
    "    original['rot'] = original['rot'].shift(100)\n",
    "    original['heading'] = original['heading'].shift(100)\n",
    "    original['navstat'] = original['navstat'].shift(100)\n",
    "    original = original.dropna()\n",
    "\n",
    "    return original\n",
    "\n",
    "def three_days_past_course(original):\n",
    "\n",
    "    original= original.reset_index(drop=True)\n",
    "\n",
    "    original['prev_lat'] = original['latitude'].shift(150)\n",
    "    original['prev_lon'] = original['longitude'].shift(150)\n",
    "    original['time_2'] = original['time'].shift(150)\n",
    "    original['cog'] = original['cog'].shift(150)\n",
    "    original['sog'] = original['sog'].shift(150)\n",
    "    original['rot'] = original['rot'].shift(150)\n",
    "    original['heading'] = original['heading'].shift(150)\n",
    "    original['navstat'] = original['navstat'].shift(150)\n",
    "    original = original.dropna()\n",
    "\n",
    "    return original\n",
    "\n",
    "def add_data(original,train):\n",
    "    ships = original['vesselId'].unique()\n",
    "    new = original.copy()\n",
    "    new['time'] = pd.to_datetime(new['time'])\n",
    "    new['prev_lat']=original['latitude']\n",
    "    new['prev_lon']=original['longitude']\n",
    "    new['time_2'] = new['time']\n",
    "    ex = train.copy()\n",
    "    for c in ships:\n",
    "        one_ship = new[new['vesselId'] == c].copy()\n",
    "        new_filtered = two_days_past_course(one_ship)\n",
    "        ex = pd.concat([ex, new_filtered], ignore_index=True)\n",
    "    for c in ships:\n",
    "        one_ship = new[new['vesselId'] == c].copy()\n",
    "        new_filtered = three_days_past_course(one_ship)\n",
    "        ex = pd.concat([ex, new_filtered], ignore_index=True)\n",
    "    ex['time_dif'] = (ex['time']-ex['time_2']).dt.total_seconds()/3600\n",
    "    ex = ex[ex['time_dif'] <= 125]\n",
    "    ex = ex.reset_index(drop=True)\n",
    "\n",
    "    return ex\n",
    "\n",
    "def adapting_training_data(original):\n",
    "    ships = original['vesselId'].unique()\n",
    "    new = original.copy()\n",
    "    new['time'] = pd.to_datetime(new['time'])\n",
    "    new['prev_lat']=original['latitude']\n",
    "    new['prev_lon']=original['longitude']\n",
    "    new['time_2'] = new['time']\n",
    "    new = new.reset_index()\n",
    "    final = pd.DataFrame(columns=new.columns)\n",
    "    for c in ships:\n",
    "        one_ship = new[new['vesselId'] == c].copy()\n",
    "        new_filtered = past_course(one_ship)\n",
    "        final = pd.concat([final, new_filtered], ignore_index=True)\n",
    "    final = final.sort_values(by='index')\n",
    "    final = final.drop(['index'],axis=1)\n",
    "    final = final.reset_index(drop=True)\n",
    "    final = final.drop(['level_0'],axis=1)\n",
    "    final['time_dif'] = (final['time']-final['time_2']).dt.total_seconds()/3600\n",
    "\n",
    "    return(final)\n",
    "\n",
    "def adapting_test_data (evaluation,training):\n",
    "    evalu=evaluation.copy()\n",
    "    evalu['time'] = pd.to_datetime(evalu['time'])\n",
    "    evalu['time_2'] = evalu['time']\n",
    "    evalu[['cog','sog','heading','navstat','latitude','longitude']] = 0.1\n",
    "    evalu['etaRaw'] = evalu['time']\n",
    "    evalu['portId'] ='1'\n",
    "\n",
    "    train=training.copy()\n",
    "    train['time'] = pd.to_datetime(train['time'])\n",
    "    train['time_2'] = train['time']\n",
    "    evalu = evalu.drop(['ID','scaling_factor'],axis=1)\n",
    "\n",
    "    final = pd.concat([train, evalu], ignore_index=True)\n",
    "    x = adapting_training_data(final)\n",
    "    ships = x['vesselId'].unique()\n",
    "    last_one = pd.DataFrame(columns=x.columns)\n",
    "    for c in ships:\n",
    "        one_ship = x[x['vesselId'] == c].copy()\n",
    "        one_ship=one_ship.reset_index()\n",
    "        one_ship['etaRaw'] = one_ship['etaRaw'].shift(1)\n",
    "        one_ship['portId'] = one_ship['portId'].shift(1)\n",
    "        last_one = pd.concat([last_one, one_ship], ignore_index=True)\n",
    "    last_one = last_one.sort_values(by='index')\n",
    "    last_one = last_one.drop(['index'],axis=1)\n",
    "    last_one = last_one.reset_index(drop=True)\n",
    "    evalu = last_one.iloc[len(train):]\n",
    "    ships = evalu['vesselId'].unique()\n",
    "    final = final.iloc[0:0]\n",
    "    for c in ships:\n",
    "        filtered = evalu[evalu['vesselId'] == c].copy()\n",
    "        filtered = filtered.reset_index()\n",
    "        filtered[['time_2','cog','sog','rot','heading','navstat','etaRaw','prev_lat','prev_lon','portId']] = filtered.iloc[0][['time_2','cog','sog','rot','heading','navstat','etaRaw','prev_lat','prev_lon','portId']]\n",
    "        final = pd.concat([final,filtered], ignore_index=True)\n",
    "    \n",
    "    final['time_dif'] = (final['time']-final['time_2']).dt.total_seconds()/3600\n",
    "    final = final.sort_values(by='index')\n",
    "    final = final.drop(['index'],axis=1)\n",
    "    final = final.reset_index(drop=True)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =  adapting_test_data(X_evaluation,X_original)\n",
    "train = adapting_training_data(X_original)\n",
    "train = add_data(X_original, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['time_to_destiny'] = (train['etaRaw']-train['time']).dt.total_seconds()/3600\n",
    "test['time_to_destiny'] = (test['etaRaw']-test['time']).dt.total_seconds()/3600\n",
    "extra_ports.rename(columns={'latitude': 'port_lat','longitude': 'port_lon'}, inplace=True)\n",
    "extra_ports = extra_ports.drop(['name','portLocation','UN_LOCODE','countryName','ISO'],axis=1)\n",
    "train = pd.merge(train, extra_ports, on='portId', how='inner')\n",
    "test = pd.merge(test, extra_ports, on='portId', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full = test.copy()\n",
    "test_full = test_full.drop(['cog','sog','rot','heading','navstat','time','etaRaw','vesselId','portId','time_2','latitude','longitude'],axis=1)\n",
    "train_full = train.copy()\n",
    "train_full = train.drop(['cog','sog','rot','heading','navstat','time','etaRaw','vesselId','portId','time_2'],axis=1)\n",
    "train_lon = train_full.loc[:,['longitude']]\n",
    "train_lat = train_full.loc[:,['latitude']]\n",
    "train_full = train_full.drop(['latitude','longitude'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_lon = xgb.XGBRegressor(eval_metric='rmse')\n",
    "forest_lon.fit(train_full, train_lon.values.ravel())\n",
    "forest_lat = xgb.XGBRegressor(eval_metric='rmse')\n",
    "forest_lat.fit(train_full, train_lat.values.ravel())\n",
    "longitude_predicted = forest_lon.predict(test_full)\n",
    "latitude_predicted = forest_lat.predict(test_full)\n",
    "\n",
    "\n",
    "forest_prediction = pd.DataFrame({'longitude_predicted':longitude_predicted,'latitude_predicted':latitude_predicted})\n",
    "forest_prediction = forest_prediction.reset_index()\n",
    "forest_prediction.rename(columns={'index': 'ID'}, inplace=True)\n",
    "forest_prediction.to_csv('data/xgb_prediction_more_dates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "task1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
